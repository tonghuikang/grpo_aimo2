This document outlines my approach to fine-tuning `DeepSeek-R1-Distill-Qwen-1.5B` to classify code execution outputs generated by `DeepSeek-R1-Distill-Qwen-7B`.

However, I did not achieve improvements even in offline results. Nevertheless, I share my learnings here.

These are some hypotheses that motivate this:

- I expect that `DeepSeek-R1-Distill-Qwen-7B` is already well-tuned by DeepSeek to solve math problems from scratch. Without access to substantial GPU resources, I cannot realistically outperform DeepSeek's tuning efforts.
- However, it appears that `DeepSeek-R1-Distill-Qwen-7B` struggles to recognize whether code execution outputs make sense - possibly because the LLM hasn't been specifically trained for this task.
- Given the code execution output, if I am able to train the `DeepSeek-R1-Distill-Qwen-1.5B` to classify whether the code execution output can result in the answer, I can get a gold medal performance.

Therefore, I've narrowed the scope from solving entire math Olympiad problems to focusing solely on classifying code execution outputs. The role of `DeepSeek-R1-Distill-Qwen-1.5B` is now to decide between:

- Submitting an answer by providing it in `\boxed{}` format (which may be correct or incorrect)
- Declining to submit an answer by identifying errors and continuing with additional Python code

There are some constraints on finetuning LLMs:

- With some [free GPU credits from Modal labs](https://modal.com/pricing) from a [fine-tuning course I participated](https://maven.com/parlance-labs/fine-tuning), the biggest instance I have access to is a single 8 x H100 instance node.
- I found out that the 7B model tuned by the second prize winner [has only 1k context length](https://github.com/AIMO-CMU-MATH/CMU_MATH-AIMO/blob/main/finetune_code/scripts_aimo/finetune_policy.sh) and he used [8 x A6000 GPUs](https://github.com/AIMO-CMU-MATH/CMU_MATH-AIMO/blob/main/finetune_code/README.md) which has 48GB memory. (H100 has 80GB memory, not much more).
- However, reasoning models are severely limited with only 1k context length. Additionally, for GRPO, the group computation requires the entire group to be in [memory](https://github.com/huggingface/trl/issues/3061#issuecomment-2769820939). I decided to start with a smaller 1.5B model with approximately 4k context length - requiring at least 2 x H100 GPUs - and scale up the context length later if initial experiments prove successful.

This is how I intend to approach tuning LLMs:

- I first try to get the LLM to respond in the correct format. I have two reward functions regarding formatting - one for length and one for responding in sentences of similar length spaced by two newlines.
- After the LLM has plateaued in formatting rewards, then I introduce the correctness reward.
- The analogy here is that your table tennis coach will get you to learn the correct form first before getting you to return serves.

Dataset:

- I identified LLM generations eligible for training. The training-eligible completion string spans from the code execution output to either the end of text or the start of a Python code block ("```python"). Samples can be viewed [here](https://www.kaggle.com/code/huikang/r1-distill-qwen-tir/output?select=generation_logs_training.csv).
- I filtered for generations that are under 3000 input tokens and under 1000 output tokens - because I wanted to start training on 4k context length. 3000 input tokens should allow for some multi-step interaction, which I want the LLM to do well in.
- For each question, I pair a generation that leads to a wrong answer and another generation that leads to the correct answer.
- The full dataset is [here](https://github.com/tonghuikang/grpo_aimo2/blob/master/training_dataset.csv).

Progress:

- I successfully reproduced the [tutorial code](https://huggingface.co/docs/trl/main/en/grpo_trainer) from Huggingface's TRL library using `DeepSeek-R1-Distill-Qwen-1.5B`
- I managed to improve the LLM's performance on the formatting reward metrics
- I was unable to improve the LLM's performance on the correctness reward

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1680925%2F3313cd47a44c3461447ede65fccd2fa7%2FScreenshot%202025-04-01%20at%2021.09.46.png?generation=1743567017204241&alt=media)

The reward function is structured as:
= `style_func` + `correctness_func`
= `f(k) * (length_ref + formatting_ref)` + `(1 - f(k)) * correctness_ref`

`f(k) = 1` until 25% of training and then linearly decreases to `0.25`
The maximum value of `length_ref`, `formatting_ref`, and `correctness_ref` should be 1

Here are the plots explained:
- `length_ref` and `formatting_ref` increased and plateaued at 25% of training.
- `correctness_ref` showed no meaningful improvement. While there was an initial uptick, this occurred when no weight was assigned to `correctness_ref`. The performance was comparable to random guessing or consistently declining to submit answers - both strategies yielding a `correctness_ref` value of zero. For reference, perfect classification would produce a `correctness_ref` value of approximately 0.5.
- This is the report with more charts - https://api.wandb.ai/links/htong-quora-quora/w7hjqk8y which costs around $40 of Modal credits each run.

Learnings

- I should learn to code from Pytorch. This was what the second place winner from last year did. I still don't understand why a 1.5B model with group size of 4 and context length of 4000 is taking up one entire H100 memory. If I manage Pytorch code, I should have a better idea why. Working with Pytorch should be easier now that we can code with tools like Claude Code. 
- I should understand better exactly how logprobs are being updated.
- I should get the pipeline working on easier math problems like GSM8K first.
- Maybe I should have done SFT finetuning to imitate how the full R1 will respond to the code execution output.

The code is available here: https://github.com/tonghuikang/grpo_aimo2

Happy to take questions and elaborate more!